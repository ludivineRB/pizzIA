# -*- coding: utf-8 -*-

import chromadb
from langchain_community.chat_models import ChatOllama
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
import gradio as gr
from langchain_core.runnables import RunnablePassthrough

# --- 1. Configuration ---

# Le nom de la collection que nous avons cr√©√©e dans le script pr√©c√©dent.
COLLECTION_NAME = "recettes"
# Le mod√®le d'embedding (doit √™tre le m√™me que celui utilis√© pour la cr√©ation).
EMBEDDING_MODEL = "mxbai-embed-large"
# Le mod√®le de LLM √† utiliser pour la g√©n√©ration de la r√©ponse.
LLM_MODEL = "mistral"

# --- 2. Initialisation des composants LangChain ---

print("Initialisation des composants LangChain...")

# Initialise le client Ollama pour les embeddings
ollama_embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL)

# Initialise le client ChromaDB pour se connecter √† la base de donn√©es existante.
# Le chemin doit correspondre √† l'endroit o√π la DB a √©t√© cr√©√©e par module5_creation_db.py
# (qui est dans le m√™me dossier 'code')
vectorstore = Chroma(
    client=chromadb.PersistentClient(path="./chroma_db"),
    collection_name=COLLECTION_NAME,
    embedding_function=ollama_embeddings
)

# Cr√©e un retriever √† partir du vectorstore.
# Le retriever est responsable de la recherche des documents pertinents.
retriever = vectorstore.as_retriever(search_kwargs={"k": 5}) # R√©cup√®re les 3 chunks les plus pertinents

# Initialise le mod√®le de chat Ollama
llm = ChatOllama(model=LLM_MODEL)
pizza = [
        "documents/marco-fuso-recipe-booklet---final.pdf",
        "documents/pizza-booklet-french-68623de5495cb223587169.pdf",
        "documents/Recette-pizza-au-fromage.pdf",
        ]
allergene = [
             "documents/Recette-pizza-au-fromage.pdf",
             "documents/Tableau-des-allergenes.pdf"
             ]
# --- 3. D√©finition du prompt RAG ---

# Le template du prompt pour le LLM.
# Il inclut le contexte r√©cup√©r√© et la question de l'utilisateur.
template2 = """En tant que Polo, le meilleur pote de Marco et tu travailles dans la pizzeria de Marco Fuso, r√©ponds en te basant uniquement sur le context :
{context}

Question: {question}
"""
template = """
Tu es un expert en cuisine italienne sp√©cialis√© dans les pizzas de Marco Fuso. 
Tu dois r√©pondre uniquement en utilisant les informations suivantes extraites de documents :

{context}

Ne r√©ponds pas si la r√©ponse ne figure pas dans ces documents. Si tu ne sais pas, dis : 
"Je ne trouve pas cette information dans les documents disponibles."

Question : {question}
"""

prompt = ChatPromptTemplate.from_template(template2)

# --- 4. Construction de la cha√Æne RAG avec LangChain Expression Language (LCEL) ---

# La cha√Æne RAG est construite en utilisant LCEL pour une meilleure lisibilit√© et modularit√©.
rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()} # √âtape de recherche (Retrieval)
    | prompt                                                  # √âtape d'augmentation (Augmented)
    | llm                                                     # √âtape de g√©n√©ration (Generation)
    | StrOutputParser()                                       # Parse la sortie du LLM en cha√Æne de caract√®res
)

# --- 5. Boucle d'interaction ---

# if __name__ == "__main__":
#     print("\n--- Chatbot RAG avec LangChain ---")
#     print("Posez des questions sur le document. Tapez 'exit' pour quitter.")

#     while True:
#         user_question = input("\nVous: ")
#         if user_question.lower() == "exit":
#             break

#         print("Assistant: ...")
#         # Invoque la cha√Æne RAG avec la question de l'utilisateur
#         answer = rag_chain.invoke(user_question)
#         print(f"\rAssistant: {answer}")

# --- 5. Fonction de r√©ponse pour Gradio ---
def respond_to_question(question):
    return rag_chain.invoke(question)

# --- 6. Interface Gradio ---
with gr.Blocks() as demo:
    gr.Markdown("## üçï Chatbot Polo ‚Äì Trouvez vos allerg√®nes et recettes Marco Fuso")
    
    with gr.Row():
        with gr.Column(scale=4):
            chatbot = gr.Chatbot()
            user_input = gr.Textbox(
                placeholder="Posez une question sur les recettes ou les allerg√®nes...",
                label="Votre question"
            )
            submit_btn = gr.Button("Envoyer")

        def handle_user_input(message, history):
            response = respond_to_question(message)
            history = history + [(message, response)]
            return history, ""

        submit_btn.click(handle_user_input, inputs=[user_input, chatbot], outputs=[chatbot, user_input])
        user_input.submit(handle_user_input, inputs=[user_input, chatbot], outputs=[chatbot, user_input])

# Lancer l'interface
demo.launch()
