from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain_ollama import ChatOllama
from dataclasses import dataclass
import time

# Config LLM
LLM_MODEL = "llama3:latest"
llm = ChatOllama(model=LLM_MODEL, temperature=0.4)

# Embeddings (exemple)
embedding_model_name = "sentence-transformers/all-MiniLM-L6-v2"
embeddings = HuggingFaceEmbeddings(model_name=embedding_model_name)

# Se connecter Ã  la base ChromaDB locale ou distante
vectordb = Chroma(persist_directory="./chroma_persist", embedding_function=embeddings)

@dataclass
class AgentMessage:
    sender: str
    content: str
    timestamp: float
    message_type: str  # "task", "result", "feedback"

class BaseAgent:
    def __init__(self, name, role, specialty):
        self.name = name
        self.role = role
        self.specialty = specialty
        self.llm = llm
        self.memory = []

    def add_to_memory(self, message: AgentMessage):
        self.memory.append(message)

    def process(self, **kwargs):
        raise NotImplementedError

class RecipeParserAgent(BaseAgent):
    def __init__(self):
        super().__init__("DÃ©crypteur", "Parser", "Extraction d'information")

    def process(self, recipe_text):
        prompt = f"""
Tu es un agent expert en lecture de recettes. Analyse le texte suivant :

{recipe_text}

Ta tÃ¢che est de ressortir :
1. Le nom de la recette
2. Les ingrÃ©dients sous forme de liste
3. Les Ã©tapes de prÃ©paration

Format :
## Nom de la recette:
...

## IngrÃ©dients:
- ...
- ...

## Ã‰tapes de prÃ©paration:
1. ...
2. ...
"""
        return self.llm.invoke(prompt).content

class AllergenDetectorAgent(BaseAgent):
    def __init__(self):
        super().__init__("Allergologue", "AllergenDetector", "DÃ©tection des allergÃ¨nes")

    def process(self, ingredients_text):
        prompt = f"""
Voici une liste d'ingrÃ©dients :

{ingredients_text}

Identifie les allergÃ¨nes potentiels (gluten, lait, Å“uf, arachide, fruits Ã  coque, poisson, soja, etc.).
RÃ©ponds au format :
## AllergÃ¨nes dÃ©tectÃ©s :
- ...
- ...
"""
        return self.llm.invoke(prompt).content

class NoiseFilterAgent(BaseAgent):
    def __init__(self):
        super().__init__("Filtre", "NoiseFilter", "Suppression du texte inutile")

    def process(self, raw_text):
        prompt = f"""
Voici un texte issu d'un site de recettes :

{raw_text}

Supprime tout le contenu inutile : histoires personnelles, pubs, anecdotes. Ne garde que les informations utiles Ã  la recette.

RÃ©ponds uniquement avec le texte nettoyÃ©.
"""
        return self.llm.invoke(prompt).content

class QAAgent(BaseAgent):
    def __init__(self):
        super().__init__("RÃ©pondeur", "QuestionAnswerer", "RÃ©ponse aux questions")

    def process(self, question, recipe_clean):
        prompt = f"""
Voici une recette nettoyÃ©e :

{recipe_clean}

QUESTION : {question}

RÃ©ponds de maniÃ¨re claire, concise et utile.
"""
        return self.llm.invoke(prompt).content

class RecipeOrchestrator:
    def __init__(self, vectordb):
        self.vectordb = vectordb
        self.agents = {
            "parser": RecipeParserAgent(),
            "allergen": AllergenDetectorAgent(),
            "filter": NoiseFilterAgent(),
            "qa": QAAgent(),
        }
        self.log = []

    def log_message(self, sender, content, msg_type="result"):
        msg = AgentMessage(sender, content, time.time(), msg_type)
        self.log.append(msg)
        for agent in self.agents.values():
            agent.add_to_memory(msg)

    def retrieve_relevant_chunks(self, query, top_k=5):
        results = self.vectordb.similarity_search(query, k=top_k)
        combined_text = "\n\n".join([doc.page_content for doc in results])
        return combined_text

    def run_pipeline(self, query, question):
        results = {}

        print("ðŸ” Recherche des chunks pertinents dans ChromaDB...")
        relevant_chunks = self.retrieve_relevant_chunks(query)
        self.log_message("ChromaDB", relevant_chunks)

        print("ðŸ” Nettoyage du texte...")
        clean = self.agents["filter"].process(relevant_chunks)
        self.log_message("Filtre", clean)
        results["cleaned_text"] = clean

        print("ðŸ“‹ Parsing de la recette...")
        parsed = self.agents["parser"].process(clean)
        self.log_message("DÃ©crypteur", parsed)
        results["parsed_recipe"] = parsed

        print("âš ï¸ DÃ©tection des allergÃ¨nes...")
        allergens = self.agents["allergen"].process(parsed)
        self.log_message("Allergologue", allergens)
        results["allergens"] = allergens

        print("â“ RÃ©ponse Ã  la question...")
        answer = self.agents["qa"].process(question, parsed)
        self.log_message("RÃ©pondeur", answer)
        results["answer"] = answer

        return results

    def display_results(self, results):
        print("\n" + "="*80)
        print("ðŸ“ RÃ‰SULTATS DU SYSTÃˆME MULTI-AGENTS")
        print("="*80)
        for key, val in results.items():
            print(f"\n## {key.upper()}\n{val}\n")

def main():
    orchestrator = RecipeOrchestrator(vectordb)

    print("Bienvenue dans le systÃ¨me multi-agents avec ChromaDB")
    query = input("Tape ta requÃªte pour rÃ©cupÃ©rer des chunks pertinents (ex: 'recette tarte aux pommes'):\n> ")
    question = input("Pose ta question sur la recette:\n> ")

    results = orchestrator.run_pipeline(query, question)
    orchestrator.display_results(results)
if __name__ == "__main__":
    main()